{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"JoeyTransferLearning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"WcrQpl2mVyqz","executionInfo":{"status":"ok","timestamp":1619112472786,"user_tz":240,"elapsed":1326,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os.path\n","import torch\n","import torch.utils.data as data\n","import skimage.io as io\n","from skimage import color\n","import cv2\n","from google.colab import drive\n","from torchvision import datasets, models, transforms\n","import copy\n","from torch.optim import lr_scheduler\n","\n","### only for reference\n","import time\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from xgboost import XGBClassifier\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","import matplotlib.pyplot as plt\n","from PIL import Image"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"guNyyj4LSKI6","executionInfo":{"status":"ok","timestamp":1619112472789,"user_tz":240,"elapsed":1323,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}}},"source":["# Leave this\n","num_epochs = 10\n","\n","# Tune these\n","learning_rate = 3e-1\n","batch_size = 16 # Note: anything above 16 might cause cuda to run out of memory"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAcMqaYbV5Ux","executionInfo":{"status":"ok","timestamp":1619112472789,"user_tz":240,"elapsed":1320,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"96f49553-48bc-498f-f3bd-ffa98de27d08"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqQTuULBYWqC","executionInfo":{"status":"ok","timestamp":1619112472790,"user_tz":240,"elapsed":1317,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"4425cff6-f668-4857-9959-7553bd81d570"},"source":["import os\n","\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', 'EECS 545 Project')\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","os.chdir(GOOGLE_DRIVE_PATH)\n","print(os.getcwd())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['metadata.json', 'Image.zip', '545 Project Ideas.gdoc', 'data_scene_flow', 'Copy of Data_Loading.ipynb', '.ipynb_checkpoints', 'Image.zip (Unzipped Files)', 'Images', 'Untitled Diagram.drawio', 'Weather', 'Extra_Weather_Data', 'More_Weather_Data_60k', 'DataSplit.ipynb', 'Unet + triplet loss.ipynb', 'Data_Loading.ipynb', 'saved_model', 'Untitled', '__pycache__', 'Joey Unet and Content Loss.ipynb', 'Test version(Yuanbin)-UNet Triplet loss.ipynb', 'JoeyVAE598.py', 'ExampleInterpolation.png', 'newest Unet and Triplet Loss.ipynb', 'WeatherVAE.pt', 'Tune parameters.ipynb', 'Baselines.ipynb', 'WeatherVAE.ipynb', 'Performance', 'WeatherVAE-test (Jason).ipynb', 'JoeyTransferLearning.ipynb']\n","/content/drive/.shortcut-targets-by-id/1YO9ukO9embmobkuEcbxl71Qpmtma6P0Q/EECS 545 Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LMo_BfyJW-kD","executionInfo":{"status":"ok","timestamp":1619112472790,"user_tz":240,"elapsed":1311,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}}},"source":["### U-Net\n","def conv_block(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True)\n","    )   \n","\n","\n","class UNet(nn.Module):\n","\n","    def __init__(self, n_class):\n","        super().__init__()\n","                \n","        self.conv_down_1 = conv_block(3, 64)\n","        self.conv_down_2 = conv_block(64, 128)\n","        self.conv_down_3 = conv_block(128, 256)\n","        self.conv_down_4 = conv_block(256, 512)        \n","\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n","        \n","        self.conv_up_3 = conv_block(256 + 512, 256)\n","        self.conv_up_2 = conv_block(128 + 256, 128)\n","        self.conv_up_1 = conv_block(128 + 64, 64)\n","        \n","        self.conv_last = nn.Conv2d(64, n_class, 1)\n","        \n","        \n","    def forward(self, x):\n","        conv1 = self.conv_down_1(x)\n","        x = self.maxpool(conv1)\n","\n","        conv2 = self.conv_down_2(x)\n","        x = self.maxpool(conv2)\n","        \n","        conv3 = self.conv_down_3(x)\n","        x = self.maxpool(conv3)   \n","        \n","        x = self.conv_down_4(x)\n","        \n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv3], dim=1)\n","        \n","        x = self.conv_up_3(x)\n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv2], dim=1)       \n","\n","        x = self.conv_up_2(x)\n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv1], dim=1)   \n","        \n","        x = self.conv_up_1(x)\n","        \n","        out = self.conv_last(x)\n","\n","        return out"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgb2pAIEWVTL","executionInfo":{"status":"ok","timestamp":1619112475129,"user_tz":240,"elapsed":3648,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"e7c6ec81-91c6-4840-8874-c6351f5fbf05"},"source":["model = UNet(3)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/EECS 545 Project/saved_model/model.pkl'))  ### remember to change the path\n","model.to(\"cuda\")"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["UNet(\n","  (conv_down_1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_down_2): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_down_3): Sequential(\n","    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_down_4): Sequential(\n","    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n","  (conv_up_3): Sequential(\n","    (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_up_2): Sequential(\n","    (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_up_1): Sequential(\n","    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_last): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jO1J6OakWr5X","executionInfo":{"status":"ok","timestamp":1619112475130,"user_tz":240,"elapsed":3645,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"fa2a3e3a-67fa-4da0-abe6-ff18cf47e0fa"},"source":["print(model)\n","encoder = nn.Sequential(\n","          model.conv_down_1,\n","          model.conv_down_2,\n","          model.conv_down_3,\n","          model.conv_down_4,\n","          model.maxpool\n",")\n","encoder.requires_grad = False\n","# decoder = nn.Sequential(\n","#     nn.Conv2d(512, 64, 3)\n","# )\n","encoder.to(\"cuda\")\n","print(encoder)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["UNet(\n","  (conv_down_1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_down_2): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_down_3): Sequential(\n","    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_down_4): Sequential(\n","    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n","  (conv_up_3): Sequential(\n","    (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_up_2): Sequential(\n","    (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_up_1): Sequential(\n","    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (conv_last): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",")\n","Sequential(\n","  (0): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (1): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (2): Sequential(\n","    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (3): Sequential(\n","    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HZ0en49EXkm9","executionInfo":{"status":"ok","timestamp":1619112475305,"user_tz":240,"elapsed":3816,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}}},"source":["class SmallDataLoader:\n","  def __init__(self, BatchSize=16, TestPercent=0.1, data_loc=\"Weather\", img_size=224, dev=\"cuda\"):\n","    self.train_names = []\n","    self.train_labels = []\n","    self.val_names = []\n","    self.val_labels = []\n","    self.test_names = []\n","    self.test_labels = []\n","\n","    self.label_keys = {0: \"cloudy\", 1: \"foggy\", 2: \"rain\", 3: \"snow\", 4: \"sunny\"}\n","    self.data_loc = os.path.join(os.getcwd(), data_loc)\n","    self.batch_size = BatchSize\n","    self.img_size = img_size\n","    self.dev=dev\n","\n","    data_types = [\"cloudy\", \"foggy\", \"rain\", \"snow\", \"sunny\"]\n","    splits = [\"Train\", \"Val\", \"Test\"]\n","    for split in splits:\n","      for i in range(5):\n","        dtype = self.label_keys[i]\n","        type_loc = os.path.join(data_loc, split, dtype)\n","        files = os.listdir(type_loc)\n","\n","        if split == \"Train\":\n","          self.train_labels += len(files) * [i]\n","          self.train_names += files\n","        elif split == \"Val\":\n","          self.val_labels += len(files) * [i]\n","          self.val_names += files\n","        elif split == \"Test\":\n","          self.test_labels += len(files) * [i]\n","          self.test_names += files\n","\n","    # Shuffle data first\n","    self.N_train = len(self.train_names) - len(self.train_names) % self.batch_size\n","    self.N_val = len(self.val_names) - len(self.val_names) % self.batch_size\n","    self.N_test = len(self.test_names) - len(self.test_names) % self.batch_size\n","    self.shuffle_train()\n","\n","    self.iter_no = 0\n","    self.val_no = 0\n","\n","    self.transform_dict = {\n","        \"Train\": transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n","        transforms.GaussianBlur(5),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ]),\n","        \"Val\": transforms.Compose([\n","        transforms.CenterCrop(img_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ])\n","    }\n","\n","  # index into file names, labels\n","  def get_ind(self, index, dset=\"Train\"):\n","    if dset == \"Train\":\n","      fname = self.train_names[index]\n","      label = self.train_labels[index]\n","    elif dset == \"Val\":\n","      fname = self.val_names[index]\n","      label = self.val_labels[index]\n","\n","    label_name = self.label_keys[label]\n","    img_loc = os.path.join(self.data_loc, dset, label_name, fname)\n","    input_image = Image.open(img_loc).convert('RGB')\n","    input_tensor = self.transform_dict[dset](input_image)\n","\n","    return input_tensor, label\n","\n","  # For training\n","  def get_batch(self):\n","    # End of Epoch\n","    if self.iter_no + self.batch_size >= self.N_train:\n","      self.iter_no = 0\n","      return \"EOE\", None\n","\n","    batch_img = torch.zeros((self.batch_size, 3, self.img_size, self.img_size), device=self.dev)\n","    batch_lab = torch.zeros((self.batch_size), device=self.dev, dtype=torch.long)\n","    for i in range(self.batch_size):\n","      input_tensor, label = self.get_ind(self.iter_no + i)\n","      batch_img[i] = input_tensor\n","      batch_lab[i] = label\n","    self.iter_no += self.batch_size\n","    return batch_img, batch_lab\n","\n","  def get_val(self):\n","    if self.val_no + self.batch_size >= self.N_val:\n","      self.val_no = 0\n","      return \"EOE\", None\n","\n","    batch_img = torch.zeros((self.batch_size, 3, self.img_size, self.img_size), device=self.dev)\n","    batch_lab = torch.zeros((self.batch_size), device=self.dev, dtype=torch.long)\n","    for i in range(self.batch_size):\n","      input_tensor, label = self.get_ind(self.val_no + i, dset=\"Val\")\n","      batch_img[i] = input_tensor\n","      batch_lab[i] = label\n","    self.val_no += self.batch_size\n","    return batch_img, batch_lab\n","\n","\n","  def shuffle_train(self):\n","    inds = np.random.choice(self.N_train, size=self.N_train, replace=False)\n","    self.train_names = [self.train_names[i] for i in inds]\n","    self.train_labels = [self.train_labels[i] for i in inds]\n","    self.iter_no = 0"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qq6oYkFaXtNV","executionInfo":{"status":"ok","timestamp":1619112476608,"user_tz":240,"elapsed":5117,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"d69777bb-8c28-42db-b1ff-3bc7c9b293e6"},"source":["dl = SmallDataLoader(BatchSize=batch_size)\n","\n","decoder = nn.Sequential(\n","    # Decreasing number of filters b/c too big for linear layers rn\n","    nn.Conv2d(512, 64, 3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(5, stride=2, padding=2),\n","    nn.Conv2d(64, 8, 3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(5, stride=2, padding=2),\n","    nn.Conv2d(8, 2, 3, padding=1),\n","    nn.ReLU(),\n","    nn.MaxPool2d(5, stride=2, padding=2),\n","    # Now using linear layers\n","    nn.Flatten(),\n","    nn.Linear(2*14*14, 100),\n","    nn.Linear(100, 5)\n",")\n","\n","decoder.requires_grad = True\n","\n","# Test\n","with torch.no_grad():\n","  imgs, labels = dl.get_batch()\n","  encodings = encoder(imgs)\n","  decoder.to(\"cuda\")\n","  print(decoder(encodings).shape)\n","  print(encodings.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["torch.Size([16, 5])\n","torch.Size([16, 512, 112, 112])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kb1oCBeYZjnw","executionInfo":{"status":"ok","timestamp":1619112476609,"user_tz":240,"elapsed":5114,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"7ffa64b1-cb93-45ff-dd03-e4eebd8e6cb0"},"source":["weather_model = nn.Sequential(\n","    encoder,\n","    decoder\n",")\n","\n","# The model\n","print(weather_model)\n","# Making sure it is frozen\n","print(weather_model[0].requires_grad)\n","print(weather_model[1].requires_grad)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Sequential(\n","  (0): Sequential(\n","    (0): Sequential(\n","      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","    )\n","    (1): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","    )\n","    (2): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","    )\n","    (3): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","    )\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (1): Sequential(\n","    (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): ReLU()\n","    (5): MaxPool2d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n","    (6): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU()\n","    (8): MaxPool2d(kernel_size=5, stride=2, padding=2, dilation=1, ceil_mode=False)\n","    (9): Flatten(start_dim=1, end_dim=-1)\n","    (10): Linear(in_features=392, out_features=100, bias=True)\n","    (11): Linear(in_features=100, out_features=5, bias=True)\n","  )\n",")\n","False\n","True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a_in29IRckIb","executionInfo":{"status":"ok","timestamp":1619112476609,"user_tz":240,"elapsed":5110,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}}},"source":["# Now try training it!\n","def train_model(model, criterion, optimizer, scheduler, data_loader, num_epochs=10):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    train_hist = []\n","    val_hist = []\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            data_loader.shuffle_train()\n","            while True:\n","                if phase == \"train\":\n","                  inputs, labels = data_loader.get_batch()\n","                else:\n","                  inputs, labels = data_loader.get_val()\n","                # End of epoch\n","                if inputs == \"EOE\":\n","                  break\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","            if phase == \"train\":\n","              epoch_loss = running_loss / data_loader.N_train\n","              epoch_acc = running_corrects / data_loader.N_train\n","            else:\n","              epoch_loss = running_loss / data_loader.N_val\n","              epoch_acc = running_corrects / data_loader.N_val\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","            if phase == \"train\":\n","              train_hist.append(epoch_acc)\n","            else:\n","              val_hist.append(epoch_acc)\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, train_hist, val_hist"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pOFMfhecqaB","outputId":"4ddf23e7-3440-4f4d-f501-2284b0d108a6"},"source":["criterion = nn.CrossEntropyLoss()\n","\n","# Only the decoder is being optimized\n","optimizer_ft = optim.Adam(weather_model[1].parameters(), lr=learning_rate)\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.1)\n","\n","model, train_hist, val_hist = train_model(weather_model, criterion, optimizer_ft, exp_lr_scheduler, dl, num_epochs=num_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0/9\n","----------\n","train Loss: 7507355.0743 Acc: 0.2146\n","val Loss: 5.9536 Acc: 0.1953\n","Epoch 1/9\n","----------\n","train Loss: 2.0287 Acc: 0.2271\n","val Loss: 1.8644 Acc: 0.2227\n","Epoch 2/9\n","----------\n","train Loss: 1.5788 Acc: 0.2118\n","val Loss: 1.5738 Acc: 0.1953\n","Epoch 3/9\n","----------\n"],"name":"stdout"}]}]}