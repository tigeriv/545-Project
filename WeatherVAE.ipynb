{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"WeatherVAE.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"id":"OKXSEQjRh63r"},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YgYSqQVHEWh"},"source":["# Leave this\n","img_size = 224\n","num_epochs = 10\n","\n","# TUNE THESE\n","latent_size = 50\n","hidden_size = 100\n","learning_rate = 3e-4\n","batch_size = 16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evqEGDXRipC-","executionInfo":{"status":"ok","timestamp":1619104134943,"user_tz":240,"elapsed":557,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"c7e9371a-b4c3-4ffc-e87c-31b96f0c3f46"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8OU2pRkivyc","executionInfo":{"status":"ok","timestamp":1619104134943,"user_tz":240,"elapsed":551,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"1393acc7-3972-49c9-c4ca-395f6fad94af"},"source":["import os\n","\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', 'EECS 545 Project')\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","os.chdir(GOOGLE_DRIVE_PATH)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['metadata.json', 'Image.zip', '545 Project Ideas.gdoc', 'data_scene_flow', 'Copy of Data_Loading.ipynb', '.ipynb_checkpoints', 'Image.zip (Unzipped Files)', 'Images', 'Untitled Diagram.drawio', 'Weather', 'Extra_Weather_Data', 'More_Weather_Data_60k', 'DataSplit.ipynb', 'Unet + triplet loss.ipynb', 'Data_Loading.ipynb', 'saved_model', 'Untitled', '__pycache__', 'Joey Unet and Content Loss.ipynb', 'Test version(Yuanbin)-UNet Triplet loss.ipynb', 'JoeyVAE598.py', 'ExampleInterpolation.png', 'JoeyTransferLearning.ipynb', 'newest Unet and Triplet Loss.ipynb', 'WeatherVAE.pt', 'Tune parameters.ipynb', 'Baselines.ipynb', 'WeatherVAE.ipynb']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sLdT7GSljI0f"},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import init\n","import torchvision\n","import torchvision.transforms as T\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","import torchvision.datasets as dset\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# for plotting\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['font.size'] = 16\n","plt.rcParams['image.interpolation'] = 'nearest'\n","# plt.rcParams['image.cmap'] = 'gray'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdQhVgi5jVQp","executionInfo":{"status":"ok","timestamp":1619104135947,"user_tz":240,"elapsed":1546,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"51ae4346-1cca-4107-8ff5-6c46b54c598d"},"source":["if torch.cuda.is_available:\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Good to go!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mExnwvTXjcF_"},"source":["import numpy as np\n","from torchvision import datasets, models, transforms\n","from PIL import Image\n","\n","class SmallDataLoader:\n","  def __init__(self, BatchSize=16, TestPercent=0.1, data_loc=\"Weather\", img_size=224, dev=\"cuda\"):\n","    self.train_names = []\n","    self.train_labels = []\n","    self.val_names = []\n","    self.val_labels = []\n","    self.test_names = []\n","    self.test_labels = []\n","\n","    self.label_keys = {0: \"cloudy\", 1: \"foggy\", 2: \"rain\", 3: \"snow\", 4: \"sunny\"}\n","    self.data_loc = os.path.join(os.getcwd(), data_loc)\n","    self.batch_size = BatchSize\n","    self.img_size = img_size\n","    self.dev=dev\n","\n","    data_types = [\"cloudy\", \"foggy\", \"rain\", \"snow\", \"sunny\"]\n","    splits = [\"Train\", \"Val\", \"Test\"]\n","    for split in splits:\n","      for i in range(5):\n","        dtype = self.label_keys[i]\n","        type_loc = os.path.join(data_loc, split, dtype)\n","        files = os.listdir(type_loc)\n","\n","        if split == \"Train\":\n","          self.train_labels += len(files) * [i]\n","          self.train_names += files\n","        elif split == \"Val\":\n","          self.val_labels += len(files) * [i]\n","          self.val_names += files\n","        elif split == \"Test\":\n","          self.test_labels += len(files) * [i]\n","          self.test_names += files\n","\n","    # Shuffle data first\n","    self.N_train = len(self.train_names) - len(self.train_names) % self.batch_size\n","    self.N_val = len(self.val_names) - len(self.val_names) % self.batch_size\n","    self.N_test = len(self.test_names) - len(self.test_names) % self.batch_size\n","    self.shuffle_train()\n","\n","    self.iter_no = 0\n","    self.val_no = 0\n","\n","    self.transform_dict = {\n","        \"Train\": transforms.Compose([\n","        transforms.RandomResizedCrop(self.img_size),\n","        transforms.RandomHorizontalFlip(),\n","        # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n","        # transforms.GaussianBlur(5),\n","        transforms.ToTensor()\n","        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ]),\n","        \"Val\": transforms.Compose([\n","        transforms.CenterCrop(img_size),\n","        transforms.ToTensor()\n","        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ])\n","    }\n","\n","  # index into file names, labels\n","  def get_ind(self, index, dset=\"Train\"):\n","    if dset == \"Train\":\n","      fname = self.train_names[index]\n","      label = self.train_labels[index]\n","    elif dset == \"Val\":\n","      fname = self.val_names[index]\n","      label = self.val_labels[index]\n","\n","    label_name = self.label_keys[label]\n","    img_loc = os.path.join(self.data_loc, dset, label_name, fname)\n","    input_image = Image.open(img_loc).convert('RGB')\n","    input_tensor = self.transform_dict[dset](input_image)\n","    return input_tensor.to(device=self.dev), label\n","\n","  # For training\n","  def get_batch(self):\n","    # End of Epoch\n","    if self.iter_no + self.batch_size >= self.N_train:\n","      self.iter_no = 0\n","      return \"EOE\", None\n","\n","    batch_img = torch.zeros((self.batch_size, 3, self.img_size, self.img_size), device=self.dev, dtype=torch.float32)\n","    batch_lab = torch.zeros((self.batch_size), device=self.dev, dtype=torch.long)\n","    for i in range(self.batch_size):\n","      input_tensor, label = self.get_ind(self.iter_no + i)\n","      batch_img[i] += input_tensor\n","      batch_lab[i] += label\n","    self.iter_no += self.batch_size\n","    return batch_img, batch_lab\n","\n","  def get_val(self):\n","    if self.val_no + self.batch_size >= self.N_val:\n","      self.val_no = 0\n","      return \"EOE\", None\n","\n","    batch_img = torch.zeros((self.batch_size, 3, self.img_size, self.img_size), device=self.dev)\n","    batch_lab = torch.zeros((self.batch_size), device=self.dev, dtype=torch.long)\n","    for i in range(self.batch_size):\n","      input_tensor, label = self.get_ind(self.val_no + i, dset=\"Val\")\n","      batch_img[i] = input_tensor\n","      batch_lab[i] = label\n","    self.val_no += self.batch_size\n","    return batch_img, batch_lab\n","\n","\n","  def shuffle_train(self):\n","    inds = np.random.choice(self.N_train, size=self.N_train, replace=False)\n","    self.train_names = [self.train_names[i] for i in inds]\n","    self.train_labels = [self.train_labels[i] for i in inds]\n","    self.iter_no = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wV8fbzenkAXm"},"source":["\n","## Train a model\n","\n","Now that we have our VAE defined and loss function ready, lets train our model! Our training script is provided  in `a6_helper.py`, and we have pre-defined an Adam optimizer, learning rate, and # of epochs for you to use. \n","\n","Training for 10 epochs should take ~2 minutes and your loss should be less than 120."]},{"cell_type":"code","metadata":{"id":"SHZyVXPo1J_M"},"source":["from JoeyVAE598 import loss_function\n","\n","def train_vae(epoch, model, train_loader, cond=False, num_class=5, lr=0.002):\n","    \"\"\"\n","    Train a VAE or CVAE!\n","\n","    Inputs:\n","    - epoch: Current epoch number \n","    - model: VAE model object\n","    - train_loader: PyTorch Dataloader object that contains our training data\n","    - cond: Boolean value representing whether we're training a VAE or \n","    Conditional VAE \n","    \"\"\"\n","    model.train()\n","    train_loss = 0\n","    loss = None\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    while True:\n","      data, labels = train_loader.get_batch()\n","      if data == \"EOE\":\n","        break\n","      if cond:\n","        one_hot_vec = one_hot(labels, num_classes).to(device='cuda')\n","        recon_batch, mu, logvar = model(data, one_hot_vec)\n","      else:\n","        recon_batch, mu, logvar = model(data)\n","      optimizer.zero_grad()\n","      loss = loss_function(recon_batch, data, mu, logvar)\n","      loss.backward()\n","      train_loss += loss.data\n","      optimizer.step()\n","    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWaaacNHsfao","executionInfo":{"status":"ok","timestamp":1619104405581,"user_tz":240,"elapsed":2693,"user":{"displayName":"Joseph Wilson","photoUrl":"","userId":"10418926994387796219"}},"outputId":"02efd3b1-0657-42cf-ffd9-b955cf87144f"},"source":["from JoeyVAE598 import VAE\n","\n","dl = SmallDataLoader(BatchSize=batch_size, img_size=img_size)\n","device = 'cuda'\n","\n","input_size = dl.img_size ** 2\n","model = VAE(input_size, latent_size=latent_size, hidden_size=hidden_size)\n","model.cuda()\n","\n","# Check latent size\n","with torch.no_grad():\n","  z = torch.randn(5, 3, img_size, img_size).to(device='cuda')\n","  print(model.encoder(z).shape)\n","\n","# Train and save\n","for epoch in range(0, num_epochs):\n","  print(epoch)\n","  train_vae(epoch, model, dl, lr=learning_rate)\n","  torch.save(model, 'WeatherVAE.pt')\n","\n","# Load\n","model = torch.load('WeatherVAE.pt')\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([5, 100])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["VAE(\n","  (encoder): Sequential(\n","    (0): Sequential(\n","      (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","    )\n","    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (2): Sequential(\n","      (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","    )\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Sequential(\n","      (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","    )\n","    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Flatten(start_dim=1, end_dim=-1)\n","    (7): Linear(in_features=576, out_features=100, bias=True)\n","    (8): ReLU()\n","  )\n","  (mu_layer): Linear(in_features=100, out_features=50, bias=True)\n","  (logvar_layer): Linear(in_features=100, out_features=50, bias=True)\n","  (decoder): Sequential(\n","    (0): Linear(in_features=50, out_features=100, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=100, out_features=100, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=100, out_features=150528, bias=True)\n","    (5): Unflatten(dim=1, unflattened_size=(3, 224, 224))\n","    (6): Sigmoid()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"JT6Ek-26jjJD"},"source":["## Visualize results\n","\n","After training our VAE network, we're able to take advantage of its power to generate new training examples. This process simply involves the decoder: we intialize some random distribution for our latent spaces z, and generate new examples by passing these latent space into the decoder. \n","\n","Run the cell below to generate new images! You should be able to visually recognize many of the digits, although some may be a bit blurry or badly formed. Our next model will see improvement in these results. "]},{"cell_type":"code","metadata":{"id":"RhhrsgrMTyTi"},"source":["z = torch.randn(5, latent_size).to(device='cuda')\n","import matplotlib.gridspec as gridspec\n","model.eval()\n","samples = model.decoder(z).data.cpu().numpy()\n","\n","fig = plt.figure(figsize=(5, 1))\n","gspec = gridspec.GridSpec(1, 5)\n","gspec.update(wspace=0.05, hspace=0.05)\n","for i, sample in enumerate(samples):\n","  ax = plt.subplot(gspec[i])\n","  plt.axis('off')\n","  ax.set_xticklabels([])\n","  ax.set_yticklabels([])\n","  ax.set_aspect('equal')\n","  plt.imshow(np.transpose(sample, axes=[1, 2, 0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sx3HGSpXk1MY"},"source":["## Latent Space Interpolation\n","\n","As a final visual test of our trained VAE model, we can perform interpolation in latent space. We generate random latent vectors $z_0$ and $z_1$, and linearly interplate between them; we run each interpolated vector through the trained generator to produce an image.\n","\n","Each row of the figure below interpolates between two random vectors. For the most part the model should exhibit smooth transitions along each row, demonstrating that the model has learned something nontrivial about the underlying spatial structure of the digits it is modeling."]},{"cell_type":"code","metadata":{"id":"XZ_4XsFURmN1"},"source":["def show_images(images, title=\"ExampleInterpolation.png\"):\n","    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n","\n","    fig = plt.figure(figsize=(sqrtn, sqrtn))\n","    gs = gridspec.GridSpec(sqrtn, sqrtn)\n","    gs.update(wspace=0.05, hspace=0.05)\n","\n","    for i, img in enumerate(images):\n","        ax = plt.subplot(gs[i])\n","        plt.axis('off')\n","        ax.set_xticklabels([])\n","        ax.set_yticklabels([])\n","        ax.set_aspect('equal')\n","        plt.imshow(np.transpose(img, axes=[1, 2, 0]))\n","        plt.savefig(title)\n","    return\n","\n","S = 12\n","device = 'cuda'\n","z0 = torch.randn(S,latent_size , device=device)\n","z1 = torch.randn(S, latent_size, device=device)\n","w = torch.linspace(0, 1, S, device=device).view(S, 1, 1)\n","z = (w * z0 + (1 - w) * z1).transpose(0, 1).reshape(S * S, latent_size)\n","\n","samples = model.decoder(z).data.cpu().numpy()\n","show_images(samples)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o45QLCr2A_rS"},"source":["Now try using an SVM to classify"]},{"cell_type":"code","metadata":{"id":"MTubR0D9A-B6"},"source":["from JoeyVAE598 import reparametrize\n","\n","dl = SmallDataLoader(BatchSize=1, img_size=224, dev=\"cuda\")\n","model.eval()\n","\n","def encode_train_batch(model, data):\n","  # Create latent space\n","  encoding = model.encoder(data)\n","  mu = model.mu_layer(encoding)\n","  logvar = model.logvar_layer(encoding)\n","  z = reparametrize(mu, logvar)\n","  return z\n","\n","def get_train_encodings(model, dl, N=50):\n","  X = []\n","  Y = []\n","  B = dl.batch_size\n","  i = 0\n","  while True:\n","      data, labels = dl.get_batch()\n","      if data == \"EOE\":\n","        break\n","      i += B\n","      if i % 100 == 0:\n","        print(i)\n","      data = data.to(device='cuda:0')\n","      data = torch.cat(N*[data])\n","      z = encode_train_batch(model, data)\n","      for ind in range(N):\n","        X.append(z[ind].data.cpu().numpy())\n","        Y.append(labels[0].data.cpu().numpy())\n","  return X, Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpBhv4NbI8K7"},"source":["from sklearn import svm\n","from sklearn.metrics import accuracy_score\n","\n","def get_val_encodings(model, dl):\n","  X = []\n","  Y = []\n","  B = dl.batch_size\n","  i = 0\n","  while True:\n","      data, labels = dl.get_val()\n","      if data == \"EOE\":\n","        break\n","      i += B\n","      if i % 100 == 0:\n","        print(i)\n","      data = data.to(device='cuda:0')\n","      encoding = model.encoder(data)\n","      mu = model.mu_layer(encoding)\n","      for ind in range(B):\n","        X.append(mu[ind].data.cpu().numpy())\n","        Y.append(labels[ind].data.cpu().numpy())\n","  return X, Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKaxASyiJGuC","outputId":"6622c0aa-81e1-4a75-838e-01f1b3c99e89"},"source":["dl = SmallDataLoader(BatchSize=1, img_size=224, dev=\"cuda\")\n","model.eval()\n","\n","def test_svm(model, dl, N_samples):\n","  with torch.no_grad():\n","    print(\"Getting train set\")\n","    X, Y = get_train_encodings(model, dl, N=N_samples)\n","    print(\"Fitting\")\n","    clf = svm.SVC()\n","    clf.fit(X, Y)\n","    print(\"Getting val set\")\n","    X_val, Y_val = get_val_encodings(model, dl)\n","    pred_y = clf.predict(X_val)\n","    acc = accuracy_score(np.asarray(pred_y), np.asarray(Y_val))\n","    return acc\n","\n","N_list = [1, 10, 50, 100]\n","acc_list = []\n","for N_samples in N_list:\n","  acc = test_svm(model, dl, N_samples)\n","  acc_list.append(acc * 100)\n","  print(\"N:\", N_samples, \" Accuracy:\", acc*100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","100\n","200\n","N: 1  Accuracy: 32.421875\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","100\n","200\n","N: 10  Accuracy: 29.296875\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","100\n","200\n","N: 50  Accuracy: 31.640625\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tNKm9YgcKMGt"},"source":["print(N_list)\n","print(acc_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ft1k3v56yokf"},"source":[""],"execution_count":null,"outputs":[]}]}